{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pertemuan 12\n",
    "- CUDA Implementation Feature Detection, Description \n",
    "- CUDA Feature Matching\n",
    "- Feature Matching + Homography to Detect Object\n",
    "- Object Tracker : \n",
    "    - Tracker CSRT\n",
    "    - Tracker KCF\n",
    "___\n",
    "### Maximizing Jetson Nano Perfomance\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sudo nvpmodel -m 0\n",
    "# sudo jetson_clocks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check OpenCV Version\n",
    "\n",
    "cv2.__version__"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'4.5.3'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. CUDA Implementation Feature Detection, Description and Feature Matching\n",
    "## 1.1 CUDA Harris Corner Detector\n",
    "- CUDA method use `cv2.cuda.createHarrisCorner(srcType, blockSize, ksize, k)`\n",
    "- Here : \n",
    "    - `srcType` : Input source type. Only `CV_8UC1` and `CV_32FC1` are supported for now.\n",
    "    - `blockSize` :  It is the size of neighbourhood considered for corner detection\n",
    "    - `ksize` : Aperture parameter of the Sobel derivative used.\n",
    "    - `k` : Harris detector free parameter in the equation.\n",
    "- Then use `.compute(src, dst)` to find corner in GPU matrix image,\n",
    "- Where :\n",
    "    - `src` : input image (GPU Mat)\n",
    "    - `dst` : output image (GPU Mat)\n",
    "\n",
    "## 1.2 CUDA Shi-Tomasi Corner Detector\n",
    "- CUDA method use `cv2.cuda.createGoodFeaturesToTrackDetector(srcType, maxCorners, qualityLevel, minDistance, blockSize, useHarrisDetector, harrisK)`,\n",
    "- Where : \n",
    "    - `srcType` : Input source type. Only `CV_8UC1` and `CV_32FC1` are supported for now.\n",
    "    - `maxCorners`: Maximum number of corners to return. If negative, means nolimit.\n",
    "    - `qualityLevel`: Parameter characterizing the minimal accepted quality of image corners. \n",
    "    - `minDistance`: Minimum possible Euclidean distance between the returned corners.\n",
    "    - `blockSize` : It is the size of neighbourhood considered for corner detection\n",
    "    - `useHarrisDetector` : Parameter indicating whether to use a Harris detector.\n",
    "    - `harrisK` : Harris detector free parameter in the equation.\n",
    "- Then user `.detect(src, dst, corners, mask)` to find corner in GPU matrix image,\n",
    "- Where :\n",
    "    - `src` : input image (GPU Mat).\n",
    "    - `corners` : corners Output vector of detected corners (1-row matrix with `CV_32FC2` type with corners positions).\n",
    "    - `mask` : Optional region of interest.\n",
    "\n",
    "## 1.3 CUDA SURF (Speeded-Up Robust Features)\n",
    "- CUDA Method use `cv2.cuda.SURF_CUDA_create(_hessianThreshold, _nOctaves, _nOctaveLayers, _extended, _keypointsRatio, _upright)`,\n",
    "- Where : \n",
    "    - `_hessianThreshold` : Threshold for hessian keypoint detector used in SURF.\n",
    "    - `_nOctaves` : Number of pyramid octaves the keypoint detector will use.\n",
    "    - `_nOctaveLayers` : Number of octave layers within each octave.\n",
    "    - `_extended` : Extended descriptor flag (true - use extended 128-element descriptors;\n",
    "    - `_upright` : Up-right or rotated features flag (true - do not compute orientation of features;\n",
    "- Then call `.detect(img, mask)` to detect **keypoint** on image using SURF.\n",
    "- Where : \n",
    "    - `img` : input image  (GPU Mat).\n",
    "    - `mask` : Optional region of interest.\n",
    "- Optionaly, we cann call `.detectWithDescriptors(img, mask)` to detect **keypoint** and **compute descrptor** at one command.\n",
    "\n",
    "## 1.4 CUDA FAST (Features from Accelerated Segment Test) \n",
    "- CUDA Method use `cv2.cuda.FastFeatureDetector_create(threshold, nonmaxSuppression, type, max_npoints)`,\n",
    "- Where : \n",
    "    - `threshold` : default 10.\n",
    "    - `nonmaxSuppression` : default `true`.\n",
    "    - `type` : \n",
    "        - `cv2.TYPE_5_8`\n",
    "        - `cv2.TYPE_7_12 `\n",
    "        - `cv2.TYPE_9_16`\n",
    "        - `cv2.THRESHOLD`\n",
    "        - `cv2.NONMAX_SUPPRESSION`\n",
    "        - `cv2.FAST_N`\n",
    "    - `max_npoints` : maximum detected keypoint. default 5000.\n",
    "- Then call `.detect(img, mask)` to detect **keypoint** on image using FAST.\n",
    "- Where : \n",
    "    - `img` : input image (GPU Mat).\n",
    "    - `mask` : Optional region of interest.\n",
    "- Then call `.compute(img, keypoint)` to computes the **descriptors** from the keypoints we have found.\n",
    "- Where : \n",
    "    - `img` : input image (GPU Mat).\n",
    "    - `keypoint` : detected keypoint from `.detect(img, mask)`.\n",
    "- Optionaly, we cann call `.detectAndCompute(img, mask)` to detect **keypoint** and **compute descrptor** at one command."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE CUDA Harris Corner Detection\n",
    "\n",
    "# load image\n",
    "img = cv2.imread('chessboard.png')\n",
    "h, w, c = img.shape\n",
    "\n",
    "# GPU memory initialization\n",
    "img_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img_GpuMat.create((w, h), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "gray_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray_GpuMat.create((w, h), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "dst_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "dst_GpuMat.create((w, h), cv2.CV_32FC1) # cv2.CV_32FC1 -> 32 bit float image 1 channel\n",
    "\n",
    "# create CUDA Harris COrner Detector object\n",
    "HarrisCorner = cv2.cuda.createHarrisCorner(srcType=cv2.CV_8UC1, blockSize=2, ksize=3, k=0.04)\n",
    "\n",
    "# upload to GPU memory\n",
    "img_GpuMat.upload(img)\n",
    "\n",
    "# convert to grayscale using CUDA\n",
    "cv2.cuda.cvtColor(img_GpuMat, cv2.COLOR_BGR2GRAY, gray_GpuMat)\n",
    "\n",
    "# apply CUDA Harris Corner Detector\n",
    "HarrisCorner.compute(gray_GpuMat, dst_GpuMat)\n",
    "\n",
    "# download to host memory\n",
    "dst = dst_GpuMat.download() \n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# build mask image to store local maxima coordinate\n",
    "mask = np.zeros((h,w), np.uint8)\n",
    "mask[dst>0.05*dst.max()] = 255\n",
    "\n",
    "# find contour from detected image corner \n",
    "contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# draw cicle on detected contour\n",
    "for cnt in contours :\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    cv2.circle(img, (x + w//2, y + h//2), int(0.02*img.shape[0]), (0, 255, 0), 2)\n",
    "\n",
    "# show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img[:,:,::-1])\n",
    "plt.title(\"Detected Corner\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.title(\"Corner Haris Image after Dillating (type CV_32FU1)\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE CUDA Shi-Tomasi Corner Detection\n",
    "\n",
    "# load image\n",
    "img = cv2.imread('chessboard.png')\n",
    "h, w, c = img.shape\n",
    "\n",
    "# GPU memory initialization\n",
    "img_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img_GpuMat.create((w, h), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "gray_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray_GpuMat.create((w, h), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "\n",
    "# create CUDA SURF\n",
    "GoodFeature = cv2.cuda.createGoodFeaturesToTrackDetector(srcType=cv2.CV_8UC1, maxCorners=100, qualityLevel=0.001, minDistance=20)\n",
    "\n",
    "# upload to GPU memory\n",
    "img_GpuMat.upload(img)\n",
    "\n",
    "# convert to grayscale using CUDA\n",
    "cv2.cuda.cvtColor(img_GpuMat, cv2.COLOR_BGR2GRAY, gray_GpuMat)\n",
    "\n",
    "# apply CUDA Shi-Tomasi Corner Detector\n",
    "corners_GpuMat = GoodFeature.detect(gray_GpuMat)\n",
    "\n",
    "# download to host memory\n",
    "corners = corners_GpuMat.download() \n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# convert to int 64\n",
    "corners = corners.astype(np.int0)\n",
    "\n",
    "# draw circel for all detected corners (1, n, 2)\n",
    "for x, y in corners[0, :]:\n",
    "    cv2.circle(img, (x,y), int(0.02*img.shape[0]), (0, 0, 255), 2)\n",
    "\n",
    "# show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(img[:,:,::-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE CUDA SURF (Speeded-Up Robust Features)\n",
    "\n",
    "# load image\n",
    "img = cv2.imread('butterfly.jpg')\n",
    "h, w, c = img.shape\n",
    "\n",
    "# GPU memory initialization\n",
    "img_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img_GpuMat.create((w, h), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "gray_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray_GpuMat.create((w, h), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "\n",
    "# create CUDA SURF (Speeded-Up Robust Features) object\n",
    "SURF_Detector = cv2.cuda.SURF_CUDA_create(_hessianThreshold=40000, _upright=True)\n",
    "\n",
    "# upload to GPU memory\n",
    "img_GpuMat.upload(img)\n",
    "\n",
    "# convert to grayscale using CUDA\n",
    "cv2.cuda.cvtColor(img_GpuMat, cv2.COLOR_BGR2GRAY, gray_GpuMat)\n",
    "\n",
    "# apply CUDA SURF (Speeded-Up Robust Features) to find keypoint and descriptor\n",
    "kp_GpuMat, des_GpuMat = SURF_Detector.detectWithDescriptors(gray_GpuMat, None)\n",
    "\n",
    "# download to host memory\n",
    "# Keypoint GPU Mat need to use `.downloadKeypoints()` from SURF object, \n",
    "# because it needs to deserialize a GPUMat int a std::vector<KeyPoint>\n",
    "kp = SURF_Detector.downloadKeypoints(kp_GpuMat)\n",
    "des = des_GpuMat.download()\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# dray keypoints (detected corners by SIFT)\n",
    "img = cv2.drawKeypoints(img, kp, img, (255,0,0), 4)\n",
    "\n",
    "print(\"descriptor shape \", des.shape)\n",
    "\n",
    "# show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(img[:,:,::-1])\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE CUDA FAST (Features from Accelerated Segment Test) \n",
    "\n",
    "# load image\n",
    "img = cv2.imread('butterfly.jpg')\n",
    "h, w, c = img.shape\n",
    "\n",
    "# GPU memory initialization\n",
    "img_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img_GpuMat.create((w, h), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "gray_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray_GpuMat.create((w, h), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "\n",
    "# create CUDA FAST (Features from Accelerated Segment Test) object\n",
    "FAST_Detector = cv2.cuda.FastFeatureDetector_create(threshold=40)\n",
    "\n",
    "# upload to GPU memory\n",
    "img_GpuMat.upload(img)\n",
    "\n",
    "# convert to grayscale using CUDA\n",
    "cv2.cuda.cvtColor(img_GpuMat, cv2.COLOR_BGR2GRAY, gray_GpuMat)\n",
    "\n",
    "# apply CUDA FAST (Features from Accelerated Segment Test) to find keypoint and descriptor\n",
    "kp_GpuMat, des_GpuMat = FAST_Detector.detectAndCompute(gray_GpuMat, None)\n",
    "\n",
    "# download to host memory\n",
    "# Keypoint GPU Mat need to use `.convert()` from FAST object, \n",
    "# because it needs to deserialize a GPUMat int a std::vector<KeyPoint>\n",
    "kp = FAST_Detector.convert(kp_GpuMat)\n",
    "des = des_GpuMat.download()\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# dray keypoints (detected corners by SIFT)\n",
    "img = cv2.drawKeypoints(img, kp, img, (255,0,0), 4)\n",
    "\n",
    "print(\"descriptor shape \", des.shape)\n",
    "\n",
    "# show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(img[:,:,::-1])\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "____\n",
    "# 2. CUDA Feature Matching\n",
    "## 2.1 CUDA Brute Force Matcher (BFMatcher)\n",
    "- Use method `cv2.cuda.DescriptorMatcher_createBFMatcher(normType)`\n",
    "- Where : \n",
    "    - `normType` : One of `cv2.NORM_L1`, `cv2.NORM_L2`, `cv2.NORM_HAMMING`. L1 and L2 norms are preferable choices for **SIFT** and **SURF** descriptors.\n",
    "- Then use `cv2.cuda.knnMatch((queryDescriptors, trainDescriptors, k, mask)`,\n",
    "- Where : \n",
    "    - `queryDescriptors` : Query set of descriptors.\n",
    "    - `trainDescriptors` : Train set of descriptors. This set is not added to the train descriptors collection stored in the class object.\n",
    "    - `k` : Count of best matches found per each query descriptor or less if a query descriptor has less than k possible matches in total.\n",
    "    - `mask` : Mask specifying permissible matches between an input query and train matrices of descriptors.\n",
    "## 2.2 NO CUDA FLANN (Fast Library for Approximate Nearest Neighbors) Implemented yet!\n",
    "- There is no CUDA FLANN Matcher availbale until now in OpenCV (v4.5.3)\n",
    "- if we still want to use FLANN matcher, just download descriptior into host memory and apply method `.knnMatch(des1, des2, k)` in `cv2.FlannBasedMatcher()`\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE CUDA SURF + CUDA Brute Force Matcher (BFMatcher)\n",
    "\n",
    "# load image\n",
    "img1 = cv2.imread('box.png')\n",
    "img2 = cv2.imread('box_in_scene.png')\n",
    "h1, w1, c1 = img1.shape\n",
    "h2, w2, c2 = img2.shape\n",
    "\n",
    "# GPU memory initialization\n",
    "img1_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img1_GpuMat.create((w1, h1), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "img2_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img2_GpuMat.create((w2, h2), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "gray1_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray1_GpuMat.create((w1, h1), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "gray2_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray2_GpuMat.create((w2, h2), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "\n",
    "# create CUDA SURF (Speeded-Up Robust Features) object\n",
    "SURF_Detector = cv2.cuda.SURF_CUDA_create(_hessianThreshold=700, _upright=True)\n",
    "\n",
    "# create CUDA BF Matcher object\n",
    "BFMatcher = cv2.cuda.DescriptorMatcher_createBFMatcher()\n",
    "\n",
    "# upload to GPU memory\n",
    "img1_GpuMat.upload(img1)\n",
    "img2_GpuMat.upload(img2)\n",
    "\n",
    "# convert to grayscale using CUDA\n",
    "cv2.cuda.cvtColor(img1_GpuMat, cv2.COLOR_BGR2GRAY, gray1_GpuMat)\n",
    "cv2.cuda.cvtColor(img2_GpuMat, cv2.COLOR_BGR2GRAY, gray2_GpuMat)\n",
    "\n",
    "# apply CUDA SURF (Speeded-Up Robust Features) to find keypoint and descriptor\n",
    "kp1_GpuMat, des1_GpuMat = SURF_Detector.detectWithDescriptors(gray1_GpuMat, None)\n",
    "kp2_GpuMat, des2_GpuMat = SURF_Detector.detectWithDescriptors(gray2_GpuMat, None)\n",
    "\n",
    "# apply BF Matcher via KNN (output is list data in host memory, doesn't need to do .download() from device memory)\n",
    "matches = BFMatcher.knnMatch(des1_GpuMat, des2_GpuMat, k=2)\n",
    "\n",
    "# download to host memory\n",
    "# Keypoint GPU Mat need to use `.downloadKeypoints()` from SURF object, \n",
    "# because it needs to deserialize a GPUMat int a std::vector<KeyPoint>\n",
    "kp1 = SURF_Detector.downloadKeypoints(kp1_GpuMat)\n",
    "kp2 = SURF_Detector.downloadKeypoints(kp2_GpuMat)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"number of keypoint 1:\", len(kp1))\n",
    "print(\"number of keypoint 2:\", len(kp2))\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "# cv.drawMatchesKnn expects list of lists as matches.\n",
    "result = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "#show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE CUDA SURF + FLANN Matcher (Fast Library for Approximate Nearest Neighbors)\n",
    "\n",
    "# load image\n",
    "img1 = cv2.imread('box.png')\n",
    "img2 = cv2.imread('box_in_scene.png')\n",
    "h1, w1, c1 = img1.shape\n",
    "h2, w2, c2 = img2.shape\n",
    "\n",
    "# GPU memory initialization\n",
    "img1_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img1_GpuMat.create((w1, h1), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "img2_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img2_GpuMat.create((w2, h2), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "gray1_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray1_GpuMat.create((w1, h1), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "gray2_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray2_GpuMat.create((w2, h2), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "\n",
    "# create CUDA SURF (Speeded-Up Robust Features) object\n",
    "SURF_Detector = cv2.cuda.SURF_CUDA_create(_hessianThreshold=700, _upright=True)\n",
    "\n",
    "# upload to GPU memory\n",
    "img1_GpuMat.upload(img1)\n",
    "img2_GpuMat.upload(img2)\n",
    "\n",
    "# convert to grayscale using CUDA\n",
    "cv2.cuda.cvtColor(img1_GpuMat, cv2.COLOR_BGR2GRAY, gray1_GpuMat)\n",
    "cv2.cuda.cvtColor(img2_GpuMat, cv2.COLOR_BGR2GRAY, gray2_GpuMat)\n",
    "\n",
    "# apply CUDA SURF (Speeded-Up Robust Features) to find keypoint and descriptor\n",
    "kp1_GpuMat, des1_GpuMat = SURF_Detector.detectWithDescriptors(gray1_GpuMat, None)\n",
    "kp2_GpuMat, des2_GpuMat = SURF_Detector.detectWithDescriptors(gray2_GpuMat, None)\n",
    "\n",
    "# download to host memory\n",
    "# Keypoint GPU Mat need to use `.downloadKeypoints()` from SURF object, \n",
    "# because it needs to deserialize a GPUMat int a std::vector<KeyPoint>\n",
    "kp1 = SURF_Detector.downloadKeypoints(kp1_GpuMat)\n",
    "kp2 = SURF_Detector.downloadKeypoints(kp2_GpuMat)\n",
    "des1 =  des1_GpuMat.download()\n",
    "des2 =  des2_GpuMat.download()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# create FLANN parameters & object\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   \n",
    "FLANN = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# apply FLANN via KNN (output is list data in host memory, doesn't need to do .download() from device memory)\n",
    "matches = matches = FLANN.knnMatch(des1, des2, k=2)\n",
    "\n",
    "print(\"number of keypoint 1:\", len(kp1))\n",
    "print(\"number of keypoint 2:\", len(kp2))\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "# cv.drawMatchesKnn expects list of lists as matches.\n",
    "result = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "#show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "____\n",
    "# 3. Feature Matching + Homography to Detect Object\n",
    "# 3.1 Understanding Homography \n",
    "- **Homography** or **Matrix Homography** is relate to Geometric Transformation in the ch-8.\n",
    "- Matrix Homography is a similar to **Perspective Transform**<br><br>\n",
    "    ![](resources/homography_transformation.jpg)<br><br>\n",
    "- in Perspective Transform we use method `cv2.getPerspectiveTransform()` to produce Perpective Matrix from the original point with a *known good set of point*.\n",
    "- The two sets of 4 points in Perspective Transform must correspond to each other in the same order and each point must be co-planar.\n",
    "- in Homography Thansform, this part much easier, \n",
    "- You can pass in 2 sets of >=4 points you suspect suspect to be “good” and it will use algorithms like **RANSAC** to find the best perspective transform between them. <br><br><br><br>\n",
    "- In OpenCV we can use method `cv2.findHomography(srcPoints, dstPoints, method, ransacReprojThreshold, mask, maxIters, confidence)`\n",
    "- Where : \n",
    "    - `srcPoints` Coordinates of the points in the original plane, a matrix of the type `CV_32FC2`.\n",
    "    - `dstPoints` Coordinates of the points in the target plane, a matrix of the type `CV_32FC2`.\n",
    "    - `method` Method used to compute a homography matrix. The following methods are possible:\n",
    "        - `0` - a regular method using all the points, i.e., *the least squares method*.\n",
    "        - `cv2.RANSAC` - RANSAC-based robust method.\n",
    "        - `cv2.LMEDS` - Least-Median robust method.\n",
    "        - `cv2.RHO` - PROSAC-based robust method.\n",
    "    - `ransacReprojThreshold` Maximum allowed reprojection error to treat a point pair as an inlier (used in the RANSAC and RHO methods only).\n",
    "    - `mask` Optional output mask set by a robust method ( RANSAC or LMeDS ). \n",
    "    - `maxIters` The maximum number of RANSAC iterations.\n",
    "    - `confidence` Confidence level, between 0 and 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE HOMOGRAPHY TRANSFORM\n",
    "\n",
    "# Read source image.\n",
    "im_src = cv2.imread('book2.jpg')\n",
    "# Four corners of the book in source image\n",
    "pts_src = np.array([[141, 131], [480, 159], [493, 630],[64, 601]])\n",
    "\n",
    "# Read destination image.\n",
    "im_dst = cv2.imread('book1.jpg')\n",
    "\n",
    "# Four corners of the book in destination image.\n",
    "pts_dst = np.array([[318, 256],[534, 372],[316, 670],[73, 473]])\n",
    "\n",
    "# Calculate Homography\n",
    "h, status = cv2.findHomography(pts_src, pts_dst)\n",
    "\n",
    "# Warp source image to destination based on homography\n",
    "im_out = cv2.warpPerspective(im_src, h, (im_dst.shape[1],im_dst.shape[0]))\n",
    "\n",
    "\n",
    "#show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(im_src)\n",
    "plt.title(\"Source Image\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(im_dst)\n",
    "plt.title(\"Destination Image\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(im_out)\n",
    "plt.title(\"Warped Source Image\")\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Feature Matcher and Homography to Find Object\n",
    "- In the last Feature Matching session, we found locations of some parts of an object in another cluttered image. \n",
    "- This information is sufficient to find the **object exactly** on the trainImage.\n",
    "- For that, we can use a function  `cv2.findHomography()`.\n",
    "- If we pass the set of points from both the images, it will find the perspective transformation of that object. \n",
    "- Then we can use `cv2.perspectiveTransform()` to find the object. \n",
    "- It needs atleast **four correct points** to find the transformation.\n",
    "- We have seen that there can be some possible errors while matching which may affect the result. \n",
    "- To solve this problem, algorithm uses **RANSAC** or **LEAST_MEDIAN** (which can be decided by the flags). \n",
    "- So good matches which provide correct estimation are called inliers and remaining are called outliers.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE DETECT OBJECT USING FLANN MATHCER + SIFT with HOMOGRAPHY\n",
    "\n",
    "# define minimum of match found\n",
    "MIN_MATCH_COUNT = 10\n",
    "\n",
    "# load image\n",
    "img1 = cv2.imread('box.png')          # queryImage\n",
    "img2 = cv2.imread('box_in_scene.png') # trainImage\n",
    "\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(gray1,None)\n",
    "kp2, des2 = sift.detectAndCompute(gray2,None)\n",
    "\n",
    "print(\"number of keypoint 1:\", len(kp1))\n",
    "print(\"number of keypoint 2:\", len(kp2))\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "# do a HOMOGRAPHY transform for all good keypoint \n",
    "if len(good)>MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "    # find Homography Matrix with method RANSAC\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "    \n",
    "    # apply perspective transform\n",
    "    h,w,d = img1.shape\n",
    "    pts = np.float32([[0,0], [0,h-1], [w-1, h-1], [w-1,0] ]).reshape(-1,1,2) #tl, bl, br, tr\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    \n",
    "    img2 = cv2.polylines(img2, [np.int32(dst)], True, (0, 0, 255), 2)\n",
    "\n",
    "else:\n",
    "    print( \"Not enough matches are found - %d/%d\" % (len(good), MIN_MATCH_COUNT) )\n",
    "    matchesMask = None\n",
    "\n",
    "\n",
    "# draw matches keypoint and homography area\n",
    "draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "img3 = cv2.drawMatches(img1, kp1, img2, kp2, good, None, **draw_params)\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(img3[:,:,::-1])\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE VIDEO STREAM - DETECT OBJECT USING FLANN MATHCER + SURF with HOMOGRAPHY\n",
    "\n",
    "# define minimum of match found\n",
    "MIN_MATCH_COUNT = 4\n",
    "\n",
    "# Initiate SURF detector\n",
    "surf = cv2.xfeatures2d.SURF_create(200)\n",
    "\n",
    "# FLANN parameters & Object\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# load image queryImage\n",
    "img1 = cv2.imread('nemo_template.jpg')         \n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find the keypoints and descriptors with SURF\n",
    "kp1, des1 = surf.detectAndCompute(gray1, None)\n",
    "\n",
    "cap = cv2.VideoCapture(\"nemo_video.mp4\")\n",
    "\n",
    "times = []\n",
    "while cap.isOpened() :\n",
    "    e1 = cv2.getTickCount()\n",
    "    ret, img2 = cap.read() # trainImage\n",
    "    if not ret : \n",
    "        break\n",
    "    img2 = cv2.resize(img2, (0,0), fx=0.5, fy=0.5)\n",
    "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # find the keypoints and descriptors with SURF\n",
    "    kp2, des2 = surf.detectAndCompute(gray2, None)\n",
    "\n",
    "    # apply FLANN Matcher\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    # do a HOMOGRAPHY transform for all good keypoint \n",
    "    try :\n",
    "        if len(good)>MIN_MATCH_COUNT:\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "            # find Homography Matrix with method RANSAC\n",
    "            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            matchesMask = mask.ravel().tolist()\n",
    "            \n",
    "            # apply perspective transform\n",
    "            h,w,d = img1.shape\n",
    "            pts = np.float32([[0,0], [0,h-1], [w-1, h-1], [w-1,0] ]).reshape(-1,1,2) #tl, bl, br, tr\n",
    "            dst = cv2.perspectiveTransform(pts,M) # object box \n",
    "            \n",
    "            # draw object box (red color)\n",
    "            img2 = cv2.polylines(img2, [np.int32(dst)], True, (0, 0, 255), 2)\n",
    "            #print( \"Matches found - %d/%d\" % (len(good), MIN_MATCH_COUNT) )\n",
    "\n",
    "        else:\n",
    "            #print( \"Not enough matches are found - %d/%d\" % (len(good), MIN_MATCH_COUNT) )\n",
    "            matchesMask = None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # show frame\n",
    "    cv2.imshow(\"detected object\", img2)\n",
    "\n",
    "    if (cv2.waitKey(1) == ord(\"q\")):\n",
    "        break\n",
    "    e2 = cv2.getTickCount()\n",
    "    times.append((e2 - e1)/ cv2.getTickFrequency())\n",
    "\n",
    "avg_time = np.array(times).mean()\n",
    "print(\"Average processing time : %.4fs\" % avg_time)\n",
    "print(\"Average FPS : %.2f\" % (1/avg_time))\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "____\n",
    "## [NOTE] Create Template at Runtime\n",
    "- On above implementation, we need to create a image template as a query image who want to detect on video frame.\n",
    "- In asddition, there is OpenCV method to help as creating ROI on video frame and use them as an image template.\n",
    "- We can use `cv2.selectROI(window_name, img, showCrosshair=False)`\n",
    "- Where : \n",
    "    - `windowName` name of the window where selection process will be shown.\n",
    "    - `img` image to select a ROI.\n",
    "    - `showCrosshair` if true crosshair of selection rectangle will be shown."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE (CUDA) VIDEO STREAM - DETECT OBJECT USING BF MATHCER + SURF with HOMOGRAPHY\n",
    "# + GSreamer (NVIDIA Accelerated element)\n",
    "# + OpenGL (Optimized Image Rendering)\n",
    "\n",
    "# create window with OpenGL enable\n",
    "window_name = \"Detected Object\"\n",
    "cv2.namedWindow(window_name, flags=cv2.WINDOW_OPENGL)    # with OpenGL\n",
    "#cv2.namedWindow(window_name)         # without OpenGL\n",
    "\n",
    "\n",
    "# load GStreamer File Loader \n",
    "from gst_file import gst_file_loader\n",
    "\n",
    "# load video file using GStreamer\n",
    "cap = cv2.VideoCapture(gst_file_loader(\"nemo_video.mp4\"), cv2.CAP_GSTREAMER)    \n",
    "#cap = cv2.VideoCapture(\"nemo_video.mp4\")\n",
    "\n",
    "# define minimum of match found\n",
    "MIN_MATCH_COUNT = 10\n",
    "\n",
    "# load image queryImage\n",
    "__, img1 = cap.read()\n",
    "\n",
    "#img1 = cv2.imread(\"nemo_template.jpg\")  \n",
    "bbox = cv2.selectROI(\"Crop for Template\", img1, False)\n",
    "x ,y ,w ,h  = np.int0(bbox)\n",
    "img1 = img1[x:x+w, y:y+h]\n",
    "\n",
    "img1 = cv2.resize(img1, (0,0), fx=2, fy=2)\n",
    "h1, w1, c1 = img1.shape \n",
    "\n",
    "__, img2 = cap.read()\n",
    "h2, w2, c2 = img2.shape\n",
    "h2, w2 = h2//2, w2//2\n",
    "\n",
    "# GPU memory initialization\n",
    "img1_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img1_GpuMat.create((w1, h1), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "img2_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img2_GpuMat.create((w2, h2), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "img2_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "img2_GpuMat.create((w2, h2), cv2.CV_8UC3) # cv2.CV_8UC3 -> 8 bit image 3 channel\n",
    "gray1_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray1_GpuMat.create((w1, h1), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "gray2_GpuMat = cv2.cuda_GpuMat() # Create GpuMat object \n",
    "gray2_GpuMat.create((w2, h2), cv2.CV_8UC1) # cv2.CV_8UC1 -> 8 bit image 1 channel\n",
    "\n",
    "\n",
    "# create CUDA SURF (Speeded-Up Robust Features) object\n",
    "SURF_Detector = cv2.cuda.SURF_CUDA_create(_hessianThreshold=200, _upright=True)\n",
    "\n",
    "# create CUDA BF Matcher object\n",
    "BFMatcher = cv2.cuda.DescriptorMatcher_createBFMatcher()\n",
    "\n",
    "\n",
    "# upload to GPU memory\n",
    "img1_GpuMat.upload(img1)\n",
    "\n",
    "# convert to grayscale using CUDA\n",
    "cv2.cuda.cvtColor(img1_GpuMat, cv2.COLOR_BGR2GRAY, gray1_GpuMat)\n",
    "\n",
    "# apply CUDA SURF (Speeded-Up Robust Features) to find keypoint and descriptor\n",
    "kp1_GpuMat, des1_GpuMat = SURF_Detector.detectWithDescriptors(gray1_GpuMat, None)\n",
    "\n",
    "# download to host memory\n",
    "kp1 = SURF_Detector.downloadKeypoints(kp1_GpuMat)\n",
    "\n",
    "times =[]\n",
    "while cap.isOpened() :\n",
    "    e1 = cv2.getTickCount()\n",
    "    ret, img2 = cap.read() # trainImage\n",
    "    if not ret : \n",
    "        break\n",
    "    img2 = cv2.resize(img2, (0,0), fx=0.5, fy=0.5)\n",
    "\n",
    "    img2_GpuMat.upload(img2)\n",
    "    \n",
    "    cv2.cuda.cvtColor(img2_GpuMat, cv2.COLOR_BGR2GRAY, gray2_GpuMat)\n",
    "\n",
    "    # apply CUDA SURF (Speeded-Up Robust Features) to find keypoint and descriptor\n",
    "    kp2_GpuMat, des2_GpuMat = SURF_Detector.detectWithDescriptors(gray2_GpuMat, None)\n",
    "\n",
    "    # apply BF Matcher via KNN (output is list data in host memory, doesn't need to do .download() from device memory)\n",
    "    matches = BFMatcher.knnMatch(des1_GpuMat, des2_GpuMat, k=2)\n",
    "\n",
    "    # download to host memory\n",
    "    kp2 = SURF_Detector.downloadKeypoints(kp2_GpuMat)\n",
    "\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    # do a HOMOGRAPHY transform for all good keypoint \n",
    "    try :\n",
    "        if len(good)>MIN_MATCH_COUNT:\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "            # find Homography Matrix with method RANSAC\n",
    "            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            matchesMask = mask.ravel().tolist()\n",
    "            \n",
    "            # apply perspective transform\n",
    "            h,w,d = img1.shape\n",
    "            pts = np.float32([[0,0], [0,h-1], [w-1, h-1], [w-1,0] ]).reshape(-1,1,2) #tl, bl, br, tr\n",
    "            dst = cv2.perspectiveTransform(pts,M) # object box \n",
    "            \n",
    "            # draw object box (red color)\n",
    "            img2 = cv2.polylines(img2, [np.int32(dst)], True, (0, 0, 255), 2)\n",
    "            #print( \"Matches found - %d/%d\" % (len(good), MIN_MATCH_COUNT) )\n",
    "\n",
    "        else:\n",
    "            #print( \"Not enough matches are found - %d/%d\" % (len(good), MIN_MATCH_COUNT) )\n",
    "            matchesMask = None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # show frame\n",
    "    #cv2.imshow(window_name, img2)\n",
    "\n",
    "    if (cv2.waitKey(1) == ord(\"q\")):\n",
    "        break\n",
    "\n",
    "    e2 = cv2.getTickCount()\n",
    "    times.append((e2 - e1)/ cv2.getTickFrequency())\n",
    "\n",
    "avg_time = np.array(times).mean()\n",
    "print(\"Average processing time : %.4fs\" % avg_time)\n",
    "print(\"Average FPS : %.2f\" % (1/avg_time))\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "# 4. Object Tracker\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# load GStreamer File Loader \n",
    "from gst_file import gst_file_loader\n",
    "\n",
    "# load video file using GStreamer \n",
    "cap = cv2.VideoCapture(gst_file_loader(\"nemo_video.mp4\"), cv2.CAP_GSTREAMER)  \n",
    "\n",
    "# Choose tracker\n",
    "#tracker = cv2.TrackerCSRT_create()\n",
    "tracker = cv2.TrackerKCF_create()\n",
    "\n",
    "___, img = cap.read()\n",
    "img = cv2.resize(img, (0,0), fx=0.5, fy=0.5)\n",
    "\n",
    "# create initial bounding box\n",
    "bbox = cv2.selectROI(\"Tracking\",img,False)\n",
    "\n",
    "tracker.init(img, bbox)\n",
    "\n",
    "while cap.isOpened():\n",
    "    e1 = cv2.getTickCount()\n",
    "    ret, img = cap.read()\n",
    "    \n",
    "    if not ret : \n",
    "        break\n",
    "\n",
    "    img = cv2.resize(img, (0,0), fx=0.5, fy=0.5)\n",
    "    success, bbox = tracker.update(img)\n",
    "\n",
    "    if success:\n",
    "        # draw bounding box\n",
    "        x ,y ,w ,h = np.int0(bbox)\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (255,0,255), 3, 1)\n",
    "        cv2.putText(img, \"Tracking\", (75, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 1)\n",
    "    else:\n",
    "        cv2.putText(img,\"Lost\", (75, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 1)\n",
    "    \n",
    "    e2 = cv2.getTickCount()\n",
    "    fps = cv2.getTickFrequency()/(e2-e1)\n",
    "    \n",
    "    cv2.putText(img,\"%d FPS \" % fps, (75,50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 1)\n",
    "    cv2.imshow(\"Tracking\",img)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "# Source\n",
    "- [https://vivekseth.com/computer-vision-matrix-differences/](https://vivekseth.com/computer-vision-matrix-differences/)\n",
    "- [https://docs.opencv.org/4.5.3/d1/de0/tutorial_py_feature_homography.html](https://docs.opencv.org/4.5.3/d1/de0/tutorial_py_feature_homography.html)\n",
    "- [https://docs.opencv.org/master/d9/dab/tutorial_homography.html](https://docs.opencv.org/master/d9/dab/tutorial_homography.html)\n",
    "- [https://learnopencv.com/homography-examples-using-opencv-python-c/](https://learnopencv.com/homography-examples-using-opencv-python-c/)\n",
    "- [https://docs.opencv.org/4.5.3/d2/d0a/tutorial_introduction_to_tracker.html](https://docs.opencv.org/4.5.3/d2/d0a/tutorial_introduction_to_tracker.html)\n",
    "- [https://docs.opencv.org/4.5.0/d9/df8/group__tracking.html](https://docs.opencv.org/4.5.0/d9/df8/group__tracking.html)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}