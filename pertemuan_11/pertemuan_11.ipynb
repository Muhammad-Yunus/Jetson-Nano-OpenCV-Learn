{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pertemuan 11\n",
    "- Feature Detection & Feature Description\n",
    "    - Harris Corner Detection\n",
    "    - Shi-Tomasi Corner Detector & Good Features to Track\n",
    "    - Introduction to SIFT (Scale-Invariant Feature Transform)\n",
    "    - Introduction to SURF (Speeded-Up Robust Features)\n",
    "    - FAST Algorithm for Corner Detection\n",
    "- Feature Matching\n",
    "    - Brute-Force based matcher\n",
    "    - FLANN based matcher\n",
    "___\n",
    "### Maximizing Jetson Nano Perfomance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sudo nvpmodel -m 0\n",
    "# sudo jetson_clocks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check OpenCV Version\n",
    "\n",
    "cv2.__version__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "# 1. Feature Detection \n",
    "- **Feature Detection** is process to find specific unique area on image like a corner or edge.\n",
    "- With this definition we understand that feature have a **large variation in intensity in all the directions** on image.<br><br>\n",
    "- At the below of image, six small image patches are given. \n",
    "- Question for you is to find the exact location of these patches in the original image. \n",
    "- How many correct results can you find?<br><br>\n",
    "    ![](resources/feature_building.jpg)<br><br>\n",
    "- A and B are flat surfaces and they are **spread over a lot of area**. \n",
    "    - It is difficult to find the exact location of these patches.\n",
    "- C and D are much more simple. \n",
    "    - They are **edges** of the building. \n",
    "    - You can find an approximate location, but exact location is still difficult. \n",
    "- E and F are some corners of the building. \n",
    "    - And they can be easily found. Because at the **corners**, wherever you move this patch, it will look different.\n",
    "<br><br><br>\n",
    "- So now we move into simpler (and widely used image) for better understanding.<br>\n",
    "    ![](resources/feature_simple.png)<br><br>\n",
    "- the **blue patch** is **flat area** and difficult to find and track. \n",
    "    - Wherever you move the blue patch it looks the same.\n",
    "- The **black patch** has an **edge**. \n",
    "    - If you move it in vertical direction (i.e. along the gradient) it **changes**. \n",
    "    - Moved along the edge (parallel to edge), it **looks the same**.\n",
    "- And for **red patch**, it is a **corner**. \n",
    "    - Wherever you move the patch, it looks different, means it is unique. \n",
    "    - So basically, corners are considered to be good features in an image. <br><br>\n",
    "- Once you have found it, you should be able to find the same in the other images. \n",
    "- How is this done? We take a region around the feature, we explain it in our own words, like \"upper part is blue sky, lower part is region from a building, on that building there is glass etc\" and you search for the same area in the other images. \n",
    "- Basically, you are `describing the feature`. \n",
    "Similarly, a computer also should describe the region around the feature so that it can find it in other images. So called description is called **Feature Description**. \n",
    "- So in this module, we are looking to different algorithms in OpenCV to **find features**, **describe them**, **match them etc**. <br><br>\n",
    "![](resources/feature-tech.png)\n",
    "<br><br>\n",
    "____\n",
    "## 1.1 Harris Corner Detection (Edge & Corner Detector)\n",
    "- **Corners** are regions in the image with **large variation in intensity** in all the directions. \n",
    "- One early attempt to find these corners was done by `Chris Harris` & `Mike Stephens` in their paper `A Combined Corner and Edge Detector` in `1988`,\n",
    "- It basically **finds the difference in intensity** for a displacement of small area with size $(u,v)$ in all directions.\n",
    "![](resources/harris.png)\n",
    "- The window function is either a rectangular window or a Gaussian window which gives weights to pixels underneath.\n",
    "- To detect Corner, we need to find maximum $E(u,v)$.\n",
    "- Then applying soring function $R$, <br>\n",
    "    ![](resources/harris_scorring.png)<br><br>\n",
    "    ![](resources/harris_diagram.png)<br><br>\n",
    "- Local maxima in 2D image ilustration (vertical value is image depth 0-255)<br>\n",
    "<img src=\"resources/local_maxima.png\" style=\"width:400px\"></img><br><br>\n",
    "- OpenCV has the function `cv2.cornerHarris(img, blockSize, ksize, k)` for this purpose. \n",
    "- Where : \n",
    "    - `img` - Input image. It should be grayscale and float32 type.\n",
    "    - `blockSize` - It is the size of neighbourhood considered for corner detection\n",
    "    - `ksize` - Aperture parameter of the Sobel derivative used.\n",
    "    - `k` - Harris detector free parameter in the equation.<br><br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load image\n",
    "img = cv2.imread('chessboard.png')\n",
    "\n",
    "# convert to grayscale with type CV_32FU1\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray32F = gray.astype(np.float32)\n",
    "\n",
    "# apply Corner Haris with blockSize=2, ksize=3, k=0.04, \n",
    "# the corner area has high intensity value compare to other area.\n",
    "dst = cv2.cornerHarris(gray32F, blockSize=2, ksize=3, k=0.04)\n",
    "\n",
    "\n",
    "# build mask image to store local maxima coordinate\n",
    "mask = np.zeros_like(gray)\n",
    "mask[dst>0.05*dst.max()] = 255\n",
    "\n",
    "# find contour from detected image corner \n",
    "contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# draw cicle on detected contour\n",
    "for cnt in contours :\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    cv2.circle(img, (x + w//2, y + h//2), int(0.02*img.shape[0]), (0, 255, 0), 2)\n",
    "\n",
    "# show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img[:,:,::-1])\n",
    "plt.title(\"Detected Corner\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.title(\"Corner Haris Image after Dillating (type CV_32FU1)\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "____\n",
    "## 1.2 Shi-Tomasi Corner Detector\n",
    "- Tomasi made a small modification to it in their paper `Good Features to Track` which shows better results compared to `Harris Corner Detector`. \n",
    "- The scoring function in Harris Corner Detector was given by:<br>\n",
    "![](resources/harris_scorring.png)<br><br>\n",
    "- Instead of this, Shi-Tomasi proposed: <br>\n",
    "![](resources/shi_scorring_.png)<br><br>\n",
    "- OpenCV has a function, `cv2.goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance, corners, mask, blockSize, useHarrisDetector, k)`. \n",
    "- It finds N strongest corners in the image by `Shi-Tomasi method` (or `Harris Corner Detection`, if you specify it). \n",
    "- Where : \n",
    "    - `image`: Input 8-bit or floating-point 32-bit, single-channel image.\n",
    "    - `maxCorners`: Maximum number of corners to return. If negative, means nolimit.\n",
    "    - `qualityLevel`: Parameter characterizing the minimal accepted quality of image corners. \n",
    "    - `minDistance`: Minimum possible Euclidean distance between the returned corners.\n",
    "    - `mask` : Optional region of interest.\n",
    "    - `blockSize` : It is the size of neighbourhood considered for corner detection\n",
    "    - `useHarrisDetector` : Parameter indicating whether to use a Harris detector.\n",
    "    - `k` : Harris detector free parameter in the equation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load image\n",
    "img = cv2.imread('blox.jpg')\n",
    "\n",
    "# convert to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect corner using Shi-Tomasi Detector\n",
    "corners = cv2.goodFeaturesToTrack(gray, maxCorners=25, qualityLevel=0.01, minDistance=10)\n",
    "\n",
    "# convert to int 64\n",
    "corners = corners.astype(np.int0)\n",
    "\n",
    "# draw circel for all detected corner\n",
    "for x, y in corners[:,0]:\n",
    "    cv2.circle(img, (x,y), int(0.02*img.shape[0]), (0, 0, 255), 2)\n",
    "\n",
    "# show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(img[:,:,::-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "____\n",
    "## 1.3 SIFT (Scale-Invariant Feature Transform)\n",
    "- In last couple of chapters, we saw some corner detectors like Harris etc. \n",
    "- They are **rotation-invariant**, which means, even if the image is rotated, we can find the same corners. \n",
    "- It is obvious because corners remain corners in rotated image also. \n",
    "- But what about scaling? A corner may not be a corner if the image is scaled. <br>     \n",
    "![](resources/sift_scale_invariant.jpg)<br><br>\n",
    "- SIFT Algorithm :\n",
    "    - 1. Scale-space Extrema Detection\n",
    "    - 2. Keypoint Localization   \n",
    "    - 3. Orientation Assignment\n",
    "    - 4. Keypoint Descriptor\n",
    "    - 5. Keypoint Matching<br><br>\n",
    "- In OpenCV we can use class `cv2.SIFT_create()` to create SIFT Object.\n",
    "- Then call method `.create(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma)` to setup parameter (if needed).\n",
    "- Where : \n",
    "    - `nfeatures` : The number of best features to retain. default is 0.\n",
    "    - `nOctaveLayers` : The number of layers in each octave. default is 3.\n",
    "    - `contrastThreshold` : The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions. default is 0.04.\n",
    "    - `edgeThreshold`: The threshold used to filter out edge-like features. default is 10. \n",
    "    - `sigma` : The sigma of the Gaussian applied to the input image.  default is 1.6.\n",
    "- Then call `.detect(img, mask)` to detect **keypoint** on image using SIFT.\n",
    "- Where : \n",
    "    - `img` : input image\n",
    "    - `mask` : optional mask image \n",
    "- Then call `.compute(img, keypoint)` to computes the **descriptors** from the keypoints we have found.\n",
    "- Where : \n",
    "    - `img` : input image\n",
    "    - `keypoint` : detected keypoint from `.detect(img, mask)`\n",
    "- Optionaly, we cann call `.detectAndCompute(img, mask)` to detect keypoint and compute descrptor at one command.\n",
    "- Also we can use `cv2.drawKeypoints(gray,kp,img, (255,0,0), 4)` to draw keypoint on image.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load image\n",
    "img = cv2.imread('chessboard.png')\n",
    "\n",
    "# convert to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# create SHIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# detect corner in image using SHIFT\n",
    "kp = sift.detect(gray,None)\n",
    "\n",
    "# dray keypoints (detected corners by SIFT)\n",
    "img = cv2.drawKeypoints(gray, kp, img, (255,0,0), 4)\n",
    "\n",
    "# compute descriptor \n",
    "# descriptor is a numpy array of shape (Number of Keypoints)Ã—128.\n",
    "kp, des = sift.compute(gray, kp)\n",
    "print(\"descriptor shape \", des.shape)\n",
    "\n",
    "# show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(img[:,:,::-1])\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "____\n",
    "## 1.4 SURF (Speeded-Up Robust Features)\n",
    "- we saw SIFT for keypoint detection and description. \n",
    "- But it was comparatively **slow** and people needed more speeded-up version. \n",
    "<br><br>\n",
    "- In OpenCV we can use class `cv2.xfeatures2d.SURF_create(hessianThreshold, nOctaves, nOctaveLayers, extended, upright)`\n",
    "- Where : \n",
    "    - `hessianThreshold` : Threshold for hessian keypoint detector used in SURF.\n",
    "    - `nOctaves` : Number of pyramid octaves the keypoint detector will use.\n",
    "    - `nOctaveLayers` : Number of octave layers within each octave.\n",
    "    - `extended` : Extended descriptor flag (true - use extended 128-element descriptors;\n",
    "    - `upright` : Up-right or rotated features flag (true - do not compute orientation of features;\n",
    "- Then use method `.detectAndCompute(img, mask)` do detect keypoint and compute descriptor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img = cv2.imread('butterfly.jpg')\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SURF object. You can specify params here or later.\n",
    "# Here I set Hessian Threshold to 400\n",
    "surf = cv2.xfeatures2d.SURF_create(400)\n",
    "\n",
    "# Check upright flag, if it False, set it to True\n",
    "surf.setUpright(True)\n",
    "\n",
    "\n",
    "# Find keypoints and descriptors directly\n",
    "kp, des = surf.detectAndCompute(gray, None)\n",
    "\n",
    "print(\"number of keypoint :\", len(kp))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `1335` keypoints is too much to show in a picture. \n",
    "- We reduce it to some `50` to draw it on an image. \n",
    "- While matching, we may need all those features, but not now. \n",
    "- So we increase the `Hessian Threshold`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check present Hessian threshold using method `.getHessianThreshold()`\n",
    "print(\"Hessian threshold :\", surf.getHessianThreshold())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We set it to some 50000. Remember, it is just for representing in picture.\n",
    "# In actual cases, it is better to have a value 300-500\n",
    "surf.setHessianThreshold(50000)\n",
    "\n",
    "# Again compute keypoints and check its number.\n",
    "kp, des = surf.detectAndCompute(gray, None)\n",
    "print(\"number of keypoint :\", len(kp))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- It is less than 50. Let's draw it on the image."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img = cv2.drawKeypoints(img, kp, None, (255,0,0), 4)\n",
    "\n",
    "#show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(img[:,:,::-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## 1.5 FAST (Features from Accelerated Segment Test) Algorithm for Corner Detection\n",
    "- We saw several feature detectors and many of them are really good. \n",
    "- But when looking from a real-time application point of view, they are not fast enough. \n",
    "- In OpenCV we can use `cv2.FastFeatureDetector_create(threshold, nonmaxSuppression, type)`\n",
    "- Where : \n",
    "    - `threshold` : default 10.\n",
    "    - `nonmaxSuppression` : default `true`.\n",
    "    - type : \n",
    "        - `cv2.TYPE_5_8`\n",
    "        - `cv2.TYPE_7_12 `\n",
    "        - `cv2.TYPE_9_16`\n",
    "        - `cv2.THRESHOLD`\n",
    "        - `cv2.NONMAX_SUPPRESSION`\n",
    "        - `cv2.FAST_N`\n",
    "- Then use method `.detect(img, mask)` to detect keypoint\n",
    "- Then use method `.compute(img, kp)` to compute descriptor\n",
    "- Or using `.detectAndCompute(img,mask)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img = cv2.imread('blox.jpg')\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initiate FAST object with default values\n",
    "fast = cv2.FastFeatureDetector_create(threshold=40)\n",
    "\n",
    "# find and draw the keypoints\n",
    "kp = fast.detect(gray, None)\n",
    "img2 = cv2.drawKeypoints(img, kp, None, color=(255,0,0))\n",
    "\n",
    "#show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(img2[:,:,::-1])\n",
    "plt.title(\"FAST default\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Disable nonmaxSuppression using method `.setNonmaxSuppression(0)`\n",
    "fast.setNonmaxSuppression(0)\n",
    "\n",
    "# re-detect keypoint using FAST\n",
    "kp = fast.detect(gray, None)\n",
    "\n",
    "# draw keypoint\n",
    "img3 = cv2.drawKeypoints(img, kp, None, color=(255,0,0))\n",
    "\n",
    "#show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(img3[:,:,::-1])\n",
    "plt.title(\"FAST disable nonmaxSuppression\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Feature Matching\n",
    "## 2.1 Basics of Brute-Force Matcher (BFMatcher)\n",
    "- Brute-Force matcher is simple. \n",
    "- It takes the descriptor of one feature in first set and is matched with all other features in second set using some distance calculation. \n",
    "- And the closest one is returned using KNN algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE BF MATCHER WITH SURF\n",
    "\n",
    "img1 = cv2.imread('box.png')\n",
    "img2 = cv2.imread('box_in_scene.png')\n",
    "\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "surf = cv2.xfeatures2d.SURF_create(400)\n",
    "surf.setUpright(True)\n",
    "surf.setHessianThreshold(7000)\n",
    "\n",
    "# Find keypoints and descriptors directly\n",
    "kp1, des1 = surf.detectAndCompute(gray1, None)\n",
    "kp2, des2 = surf.detectAndCompute(gray2, None)\n",
    "\n",
    "print(\"number of keypoint 1:\", len(kp1))\n",
    "print(\"number of keypoint 2:\", len(kp2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "# cv.drawMatchesKnn expects list of lists as matches.\n",
    "result = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "#show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# EXAMPLE BF MATCHER WITH SIFT\n",
    "\n",
    "img1 = cv2.imread('box.png')\n",
    "img2 = cv2.imread('box_in_scene.png')\n",
    "\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(gray1,None)\n",
    "kp2, des2 = sift.detectAndCompute(gray2,None)\n",
    "\n",
    "print(\"number of keypoint 1:\", len(kp1))\n",
    "print(\"number of keypoint 2:\", len(kp2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "# cv.drawMatchesKnn expects list of lists as matches.\n",
    "result = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "#show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 FLANN Matcher (Fast Library for Approximate Nearest Neighbors)\n",
    "- It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. \n",
    "- It works faster than BFMatcher for large datasets. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img1 = cv2.imread('box.png')          # queryImage\n",
    "img2 = cv2.imread('box_in_scene.png') # trainImage\n",
    "\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(gray1,None)\n",
    "kp2, des2 = sift.detectAndCompute(gray2,None)\n",
    "\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        matchesMask[i]=[1,0]\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags = cv2.DrawMatchesFlags_DEFAULT)\n",
    "result = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matches, None,**draw_params)\n",
    "\n",
    "#show result\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Source \n",
    "- [https://docs.opencv.org/4.5.3/df/d54/tutorial_py_features_meaning.html](https://docs.opencv.org/4.5.3/df/d54/tutorial_py_features_meaning.html)\n",
    "- [https://docs.opencv.org/4.5.3/dc/d0d/tutorial_py_features_harris.html](https://docs.opencv.org/4.5.3/dc/d0d/tutorial_py_features_harris.html)\n",
    "- [https://docs.opencv.org/4.5.3/d4/d8c/tutorial_py_shi_tomasi.html](https://docs.opencv.org/4.5.3/d4/d8c/tutorial_py_shi_tomasi.html)\n",
    "- [https://docs.opencv.org/4.5.3/da/df5/tutorial_py_sift_intro.html](https://docs.opencv.org/4.5.3/da/df5/tutorial_py_sift_intro.html)\n",
    "- [https://docs.opencv.org/4.5.3/df/dd2/tutorial_py_surf_intro.html](https://docs.opencv.org/4.5.3/df/dd2/tutorial_py_surf_intro.html)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 32-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}